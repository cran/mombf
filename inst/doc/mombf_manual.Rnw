\documentclass[a4paper,12pt]{article}
%\VignetteIndexEntry{Manual for the gaga library}
%\VignettePackage{gaga}

\usepackage{amsmath}    % need for subequations
\usepackage{amssymb}    %useful mathematical symbols
\usepackage{bm}         %needed for bold greek letters and math symbols
\usepackage{graphicx}   % need for PS figures
\usepackage{verbatim}   % useful for program listings
%\usepackage{color}      % use if color is used in text
\usepackage{hyperref}   % use for hypertext links, including those to external documents and URLs
\usepackage{natbib}    %number and author-year style referencing
\usepackage{epsf} 
\usepackage{lscape} 
\bibpunct{(}{)}{;}{a}{,}{,}

%Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\pagestyle{empty} % use if page numbers not wanted

\begin{document}

\title{mombf library vignette}
\author{David Rossell \\
\small{Department of Biostatistics and Bioinformatics} \\
\small{Institute for Research in Biomedicine, Barcelona, Spain} \\
\small{david.rossell@irbbarcelona.org}}
\date{}  %comment to include current date
\maketitle

This manual shows how to use the \texttt{mombf} library to compute
Moment and inverse Moment Bayes factors (Mom BF and iMom BF, respectively).
The appeal of Mom and iMom BF is that, when the null hypothesis is true,
they present better convergence rates than BF resulting from most standard procedures.
When the alternative hypothesis is true, they present the same convergence rates
as most standard procedures.

The routines compute exact BF for linear regression models,
and approximate BF for generalized linear models.
Approximate BF can also be obtained in other situations where the
regression coefficients are asymptotically normally distributed and sufficient.
The library also contains routines to evaluate the prior density and
to elicit the prior parameters by specifying the mode {\it a priori} of the standardized
regression coefficients.

In Section \ref{sec:priors} we briefly review the definition of the Mom and iMom priors,
and we present routines to evaluate them.
In Section \ref{sec:bflm} we analyze Hald's data with linear models and compute Bayes factors
to assess whether some predictors can be dropped from the model.
Section \ref{sec:bfglm} shows the analysis of some simulated logistic regression data.


\section{Mom and iMom priors}
\label{sec:priors}

Let $\bm{\theta}'=(\bm{\theta}_1',\bm{\theta}_2')$ be the vector of regression coefficients,
$\sigma^2$ be a dispersion parameter ({\it i.e.} the residual variance in a linear regression setup)
and suppose that the goal is to test $H_0: \bm{\theta}_1=\bm{\theta}_0$ versus $H_1=\bm{\theta}_1 \neq \bm{\theta}_0$.
Define the quadratic distance
$Q(\bm{\theta}_1) = (\bm{\theta}_1-\bm{\theta}_0)^T V_1^{-1} (\bm{\theta}_1-\bm{\theta}_0)/(n g \sigma^2)$, 
where $\bm{\theta}_1$ is a $p_1\times 1$ dimensional real vector, 
$V_1$ is a $p_1\times p_1$ positive definite matrix and $g>0$ is a scalar.
We set $V_1$ to be proportional to the asymptotic covariance matrix of the maximum 
likelihood estimate $\hat{\bm{\theta}}_1$.
For instance, in a linear regression setup with design matrix $X$ we set $V_1=(X'X)^{-1}$.

We define an improper prior density on $\theta_2$ proportional to 1,
and in the situation where $\sigma^2$ is unknown we specify
an independent improper prior on $\sigma^2$ proportional to $1/\sigma$.


\subsection{Mom prior}
\label{ssec:momprior}

Let $\pi_Z$ denote the g-prior of \cite{zellner:1980},
{\it i.e.} $\pi_Z(\bm{\theta}_1)=N(\bm{\theta}_0,n g \sigma^2 V_1)$.
We define the multivariate Mom prior as
\begin{equation}
\pi_M(\bm{\theta}_1)= \frac{Q(\bm{\theta}_1)^k}{E_{\pi_Z}[Q(\bm{\theta}_1)^k]}  \pi_Z(\bm{\theta}_1).
\label{multimoment}
\end{equation}
, where $E_{\pi_Z}(Q(\bm{\theta})^k)= \prod_{i=0}^{k-1} (p_1+2i)$ is the $k^{th}$ 
raw moment of a chi-square distribution with $p_1$ degrees of freedom.
Currently the library only implements the case $k=1$, {\it i.e.} $E_{\pi_Z}(Q(\bm{\theta})^k)=1$.

\subsection{iMom prior}
\label{ssec:imomprior}

The iMom prior on $\bm{\theta}_1$ is
\begin{equation}\label{eq:napd}
\pi_I(\bm{\theta}_1) = c_I \ Q(\bm{\theta}_1)^{-\frac{\nu+p_1}{2}} \exp\left[ Q(\bm{\theta}_1)^{-k} \right],
\end{equation}
where 
\begin{equation}
c_I = \left| \frac{V_1^{-1}}{n g \sigma^2} \right|^{1/2} \frac{k}{\Gamma(\nu/2k)} \frac{\Gamma(p_1/2)}{\pi^{p_1/2}}.
\end{equation}
As $Q(\bm{\theta}_1)$ increases, the influence of the 
exponential term in (\ref{eq:napd}) disappears
and the tails of $\pi_I$ are of the same order as those of a multivariate
$T$ with $\nu$ degrees of freedom.
Several authors have found appealing to set $\nu=1$ \citep{bayarri:2007},
which is the default value in our routines.
Currently the library only implements the case $k=1$.

\subsection{Evaluating the Mom and iMom priors}

The functions \texttt{dmom} and \texttt{dimom} evaluate the Mom and iMom priors,
respectively.
Let's set the prior parameter $g=1$ and plot the Mom and iMom priors in a univariate setting
for $\theta_1 \in (-3,3)$. By default $\theta_0$ is set to 0, $n=1$ and $V_1=1$.

\setkeys{Gin}{width=0.5\textwidth} 
<<label=fig1plot,include=FALSE>>=
 library(mombf)
 g <- 1
 thseq <- seq(-3,3,length=1000)
 plot(thseq,dmom(thseq,g=g),type='l',ylab='Prior density')
 lines(thseq,dimom(thseq,g=g),lty=2,col=2)
@

\begin{figure}
\begin{center}
<<label=fig1,fig=TRUE,echo=FALSE>>=
<<fig1plot>>
@
\end{center}
\caption{Moment and inverse Moment priors for $g=1$}
\label{fig:priorplot}
\end{figure}

The iMom prior density is lower than the Mom prior density 
for $\theta_1$ values that are either in a neighborhood of 0
or that are large in absolute value.


\section{Bayes factors for linear regression models}
\label{sec:bflm}

\subsection{Linear model fit and prior elicitation}
\label{ssec:lmfitandprior}

The Hald data contains 13 observations, a continuous response variable and
4 predictors. We start by loading the data and fitting a linear regression model.

<<one>>=
data(hald)
dim(hald)
lm1 <- lm(hald[,1] ~ hald[,2] + hald[,3] + hald[,4] + hald[,5])
summary(lm1)
@

The goal is to obtain Bayes factors to assess whether any one predictor
can be dropped from the model.
First, we specify the prior parameter $g$ based on considerations
about the standardized regression coefficient $\theta_1/\sigma$.
$\theta_1/\sigma$ is known as the signal-to-noise ratio,
or as the standardized effect size.
To find the $g$ value that gives a prior mode at $\pm .2$,
we use the function \texttt{mode2g.univ}.
For instance, for the regression coefficient associated to \texttt{hald[,2]}
we would do as follows.

<<two>>=
prior.mode <- .2
V <- summary(lm1)$cov.unscaled
gmom <- mode2g.univ(prior.mode,V[2,2],nrow(hald),prior='Mom')
gimom <- mode2g.univ(prior.mode,V[2,2],nrow(hald),prior='iMom')
gmom
gimom
@

We can check the obtained $g$ values by plotting the prior density.

\setkeys{Gin}{width=0.5\textwidth} 
<<label=fig2plot,include=FALSE>>=
thseq <- seq(-1,1,length=1000)
plot(thseq,dmom(thseq,V1=V[2,2],g=gmom,n=nrow(hald)),type='l',xlab='theta/sigma',ylab='Prior density')
lines(thseq,dimom(thseq,V1=V[2,2],g=gimom,n=nrow(hald)),lty=2,col=2)
abline(v=c(-prior.mode,prior.mode),lty=2)
@

\begin{figure}
\begin{center}
<<label=fig2,fig=TRUE,echo=FALSE>>=
<<fig2plot>>
@
\end{center}
\caption{Hald data. Mom and iMom priors for a regression coefficient.
The prior mode for $\theta_1/\sigma$ is set at $\pm 0.2$}
\label{fig:priorhald}
\end{figure}


\subsection{Bayes factor computation}
\label{ssec:bflmcomput}

Bayes factors can be easily computed using the functions
\texttt{mombf} and \texttt{imombf}.
The Mom BF can be computed in explicit form, whereas the
iMom BF require numerical integration.
The numerical integration is achieved via Monte Carlo simulation.
The parameter \texttt{B} passed to \texttt{imombf} specifies the 
number of Monte Carlo samples.
For computational speed, we use \texttt{B=100000} even
though in real examples a higher value can be used
to ensure proper accuracy.
For comparison, we also compute the Bayes factors that
would be obtained under Zellner's g-prior
with the default value $g=1$,
which can be achieved with the function \texttt{zellnerbf}.
For reproducibility, we set the random number generator seed
to the date this document was produced.

<<three>>=
set.seed(4*2*2008)
mombf(lm1,coef=2,g=gmom)
imombf(lm1,coef=2,g=gimom,B=10^5)
zellnerbf(lm1,coef=2,g=1)
@

We assess the Monte Carlo error by re-computing the iMom BF
with a different set of Monte Carlo samples.
We find the error to be acceptable.

<<four>>=
imombf(lm1,coef=2,g=gimom,B=10^5)
@

We now assess the sensitivity to the prior mode specification.
For illustration purposes, we exclude the iMom BF as these take longer to compute.
The estimated standardized regression coefficient is

<<five>>=
sr <- sqrt(sum(lm1$residuals^2)/(nrow(hald)-5))
thest <- coef(lm1)[2]/sr
thest
@

We define a sequence of prior modes, find the corresponding $g$ values 
and compute Bayes factors.
Note that \texttt{mombf} and \texttt{zellnerbf} accept $g$ to be a vector
instead of a single value. This is not the case for \texttt{imombf}.


\setkeys{Gin}{width=0.5\textwidth} 
<<label=fig3plot,include=FALSE>>=
prior.mode <- seq(.01,1,length=100)
gmom <- mode2g.univ(prior.mode,V[2,2],nrow(hald),prior='Mom')
bf1 <- mombf(lm1,coef=2,g=gmom)
bf2 <- zellnerbf(lm1,coef=2,g=gmom)
plot(prior.mode,bf1,type='l',ylab='BF')
lines(prior.mode,bf2,lty=2,col=2)
abline(v=thest,lty=2)
@

\begin{figure}
\begin{center}
<<label=fig3,fig=TRUE,echo=FALSE>>=
<<fig3plot>>
@
\end{center}
\caption{Hald data. BF obtained for Mom and Zellner's g-prior for several prior mode specifications.}
\label{fig:}
\end{figure}
The highest possible BF are observed when the prior mode is slightly smaller than the estimated
0.634.
As the mode converges to zero both priors converge to a point
mass at zero, and hence the BF converges to 1.
As the mode goes to infinity the BF goes to 0, as predicted by Lindley's paradox \citep{lindley:1957}.
Although the Mom and Zellner BF show some sensitivity to the
prior specification, any prior mode between 0 and 1 results in
evidence in favor of including the variable in the model.

\section{Bayes factors for generalized linear regression models}
\label{sec:bfglm}

As an illustration, we simulate data with 50 observations from a probit regression model.
We simulate two correlated predictors
with coefficients equal to \texttt{log(2)} and \texttt{0}
({\it i.e.} the second variable is not actually in the model).
The predictors are stored in the matrix \texttt{x}, the success
probabilities in the vector \texttt{p} and the observed responses 
in the vector \texttt{y}.
As in Section \ref{ssec:bflmcomput}, for reproducibility purposes
we set the random number generator seed to the date this document was produced.

<<six>>=
set.seed(4*2*2008)
n <- 50; theta <- c(log(2),0)
x <- matrix(NA,nrow=n,ncol=2)
x[,1] <- rnorm(n,0,1); x[,2] <- rnorm(n,.5*x[,1],1)
p <- pnorm(x %*% matrix(theta,ncol=1))
y <- rbinom(n,1,p)
@

Before computing Bayes factors, we fit a probit regression model with the function \texttt{glm}.
The maximum likelihood estimates are stored in \texttt{thetahat} and
the asymptotic covariance matrix in \texttt{V}.

<<seven>>=
glm1 <- glm(y~x[,1]+x[,2],family=binomial(link = "probit"))
thetahat <- coef(glm1)
V <- summary(glm1)$cov.scaled
@

To compute Bayes factors we use the functions \texttt{momknown}
and \texttt{imomknown}.
These functions take as primary arguments a vector of regression coefficients
and their covariance matrix, and hence they 
can be used in any setting where one has a statistic that
is asymptotically sufficient and normally distributed.
The resulting Bayes factors are approximate.
The functions also allow for the presence of a dispersion parameter \texttt{sigma},
{\it i.e.} the covariance of the regression coefficients is \texttt{sigma*V},
but they assume that \texttt{sigma} is known.
The probit regression model that we simulated has no over-dispersion and hence
it corresponds to \texttt{sigma=1}.
We first compare the full model with the model resulting from excluding the second covariate,
setting $g=1$ for illustration
(note that \texttt{thetahat[1]} contains the intercept).

<<eight>>=
g <- .5
bfmom.1 <- momknown(thetahat[2],V[2,2],n=n,g=g,sigma=1)
bfimom.1 <- imomknown(thetahat[2],V[2,2],n=n,nuisance.theta=2,g=g,sigma=1)
bfmom.1
bfimom.1
@
Both priors result in evidence for including the first covariate.
We now check whether the second covariate can be dropped.

<<nine>>=
bfmom.2 <- momknown(thetahat[3],V[3,3],n=n,g=g,sigma=1)
bfimom.2 <- imomknown(thetahat[3],V[3,3],n=n,nuisance.theta=2,g=g,sigma=1)
bfmom.2
bfimom.2
@
Both Mom and iMom BF provide strong evidence in favor of the simpler model,
{\it i.e.} excluding \texttt{x[,2]}.
To compare the full model with the model that has no covariates
({\it i.e.} only the constant term remains) we use the same
routines, passing a vector as the first argument 
and a matrix as the second argument.

<<ten>>=
bfmom.0 <- momknown(thetahat[2:3],V[2:3,2:3],n=n,g=g,sigma=1)
bfimom.0 <- imomknown(thetahat[2:3],V[2:3,2:3],n=n,nuisance.theta=2,g=g,sigma=1)
bfmom.0
bfimom.0
@
Based on the resulting BF being close to 1, it is not clear whether the full
model is preferable to the model with no covariates.

The BF can be used to easily compute posterior probabilities for each
of the four considered models: no covariates, only \texttt{x[,1]}, only \texttt{x[,2]}
and both \texttt{x[,1]} and \texttt{x[,2]}.
We assume equal probabilities {\it a priori}.

<<eleven>>=
prior.prob <- rep(1/4,4)
bf <- c(bfmom.0,bfmom.1,bfmom.2,1)
pos.prob <- prior.prob*bf/sum(prior.prob*bf)
pos.prob
@
The model with the highest posterior probability is the one including only \texttt{x[,1]},
{\it i.e.} the correct model,
and the model with the lowest posterior probability is that including only \texttt{x[,2]}.

\bibliographystyle{plainnat}
\bibliography{references} 

\end{document}
